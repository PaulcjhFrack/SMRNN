if __name__ == "__main__":
    sparse_taps = [0,5,6]
    sparse_time_steps = [0,5]
    num_fir = 4
    order = 3
    hidden_size_iq = 10
    output_dim = 2
    batch_size = 128
    epochs = 200
    learning_rate = 0.001
    envelope_order= 2

    path_in = r"...\in_Adj.txt"
    path_out = r""...\\out_Adj.txt"

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Device:", device)
    data_in = load_data(path_in)
    data_out = load_data(path_out)
    print(f"Data loaded shape: input={data_in.shape}, output={data_out.shape}")
    X_train, Y_train = prepare_sparse_fir_sparse_time(data_in, data_out, sparse_taps, sparse_time_steps)
    print(f"Prepared dataset shape: X={X_train.shape}, Y={Y_train.shape}")
    train_loader = create_dataloader(X_train, Y_train, batch_size)
    model = PN_RNN(sparse_taps=sparse_taps,
                   sparse_time_steps=sparse_time_steps,
                   hidden_size_iq=hidden_size_iq,
                   output_size=output_dim,
                   num_fir=num_fir,
                   envelope_order=envelope_order).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    loss_fn = torch.nn.MSELoss()
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=0.0005)
    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for batch_x, batch_y in train_loader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            optimizer.zero_grad()
            y_pred = model(batch_x)
            loss = loss_fn(y_pred, batch_y)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        avg_loss = epoch_loss / len(train_loader)
        scheduler.step(avg_loss)
        print(f"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.7f}")
    model.eval()
    with torch.no_grad():
        X_all = X_train.to(device)
        y_pred_all = model(X_all).cpu()
        nmse_val = NMSE(Y_train, y_pred_all)
        print(f"nmse_val: {nmse_val:.4f} dB")
    total_params = count_parameters(model)
    print(f"total_params: {total_params}")
    for name, param in model.named_parameters():
        if param.requires_grad:
            print(f"{name}: {param.numel()} parameters")
